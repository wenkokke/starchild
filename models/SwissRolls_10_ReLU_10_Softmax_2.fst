module SwissRolls_10_ReLU_10_Softmax_2

open StarChild.LinearAlgebra
open StarChild.Network

val layer_0 : layer 3 10
let layer_0 =
  { weights    = [ [~.0.63350075R; ~.0.07539799R; ~.0.45677415R; 0.47858751R; 0.07686371R; ~.0.53972566R; 0.05454711R; 0.60710275R; 0.58962506R; 0.16761950R]
                 ; [~.0.32554856R; ~.0.14617404R; ~.0.54564387R; ~.0.37621969R; ~.0.25100610R; 0.40747905R; 0.08077461R; ~.0.34601948R; 0.24588200R; ~.0.26563191R]
                 ; [~.0.32931378R; 0.10535967R; ~.0.68036687R; ~.0.11377822R; ~.0.64730531R; 0.40845808R; 0.64849532R; ~.0.15348633R; 0.56339455R; 0.53237706R]
                 ]
  ; biases     = [0.09139259R; ~.0.07386477R; ~.0.12301245R; ~.0.11323053R; ~.0.08543430R; ~.0.17687578R; 0.17694737R; 0.13467662R; ~.0.16066535R; ~.0.11118849R]
  ; activation = ReLU
  }

val layer_1 : layer 10 10
let layer_1 =
  { weights    = [ [0.21767262R; ~.0.27516666R; 0.33852363R; 0.51305532R; 0.26581874R; ~.0.12367616R; ~.0.44916740R; ~.0.25873441R; 0.49953526R; 0.13143153R]
                 ; [0.14114755R; ~.0.21134607R; ~.0.52897847R; 0.34649235R; 0.17356467R; 0.43980509R; ~.0.45271066R; ~.0.21616888R; ~.0.42121905R; ~.0.46616456R]
                 ; [~.0.49451932R; ~.0.41049185R; 0.25836593R; ~.0.17131777R; 0.17736639R; 0.32472193R; ~.0.47474799R; 0.19543537R; ~.0.49215373R; ~.0.22798200R]
                 ; [0.06359018R; 0.26995158R; 0.35718855R; ~.0.31855223R; 0.32549083R; ~.0.25502402R; ~.0.16993113R; 0.11148958R; ~.0.35809845R; ~.0.41198087R]
                 ; [0.50409746R; 0.56245077R; ~.0.50204015R; 0.25235876R; ~.0.16060132R; 0.49849302R; 0.06249053R; 0.23531128R; ~.0.41398871R; ~.0.42165506R]
                 ; [~.0.08445111R; 0.06387165R; 0.06498390R; ~.0.51471907R; 0.33361509R; 0.32991982R; 0.01523561R; 0.44585139R; 0.06303842R; 0.32538047R]
                 ; [~.0.24997914R; ~.0.49138007R; 0.32396749R; ~.0.31942594R; 0.03306525R; 0.12329290R; 0.20790678R; 0.32297122R; ~.0.12218516R; 0.19957916R]
                 ; [~.0.25314415R; ~.0.26262513R; ~.0.15663157R; ~.0.02283558R; ~.0.24692613R; ~.0.28623566R; 0.45346045R; ~.0.50354576R; ~.0.03079327R; ~.0.56464016R]
                 ; [~.0.46925643R; 0.24421056R; ~.0.34201247R; ~.0.40822977R; 0.18580981R; 0.23254891R; ~.0.48225453R; 0.21010479R; 0.13141239R; 0.44812280R]
                 ; [~.0.20667872R; 0.44211173R; 0.57988590R; 0.36031407R; 0.25378144R; 0.01687392R; ~.0.55107248R; ~.0.06219988R; 0.38379422R; 0.10624784R]
                 ]
  ; biases     = [0.11333594R; ~.0.15106279R; 0.21007919R; 0.12395766R; ~.0.21071115R; ~.0.21213958R; ~.0.00773190R; 0.27715781R; ~.0.20157395R; 0.23968180R]
  ; activation = ReLU
  }

val layer_2 : layer 10 2
let layer_2 =
  { weights    = [ [~.0.19836287R; 0.32396752R]
                 ; [0.15988861R; ~.0.40304282R]
                 ; [~.0.60158360R; ~.0.40128750R]
                 ; [~.0.27753216R; ~.0.00497035R]
                 ; [0.15931141R; ~.0.13794677R]
                 ; [0.56755382R; 0.06791291R]
                 ; [~.0.35697418R; ~.0.04863155R]
                 ; [~.0.66826242R; ~.0.56745666R]
                 ; [0.59104651R; 0.07021407R]
                 ; [~.0.33080310R; ~.0.01226592R]
                 ]
  ; biases     = [~.0.24258493R; 0.24258490R]
  ; activation = Softmax
  }

val model : network 3 2 3
let model = NStep NStep layer_0 (layer_1) (NLast layer_2)